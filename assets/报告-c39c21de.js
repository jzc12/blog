const n=`------

## REINFORCE ç®—æ³•å®žçŽ°æŠ¥å‘Š

### é€‰é¢˜æ„ä¹‰

| å†…å®¹é¡¹       | è¯´æ˜Ž                                                         |
| ------------ | ------------------------------------------------------------ |
| é¡¹ç›®èƒŒæ™¯ | éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²åœ¨æ¸¸æˆæ™ºèƒ½ã€è‡ªåŠ¨æŽ§åˆ¶ã€æŽ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—çªç ´ã€‚ |
| ç ”ç©¶åŠ¨æœº | REINFORCE æ˜¯æœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¹‹ä¸€ï¼Œå­¦ä¹ æˆæœ¬ä½Žï¼Œé€‚åˆå…¥é—¨ç†è§£ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ã€‚ |
| å®žç”¨ä»·å€¼ | é€šè¿‡è¯¥ç®—æ³•ï¼Œèƒ½ç›´è§‚ç†è§£â€œç­–ç•¥ç½‘ç»œ+é‡‡æ ·+å›žæŠ¥è¯„ä¼°+ä¼˜åŒ–â€è¿™ä¸€é€šç”¨ RL æ¡†æž¶ï¼Œå…·å¤‡æ•™å­¦å’Œå®žéªŒä»·å€¼ã€‚ |

------

### ðŸ›  æŠ€æœ¯ç»†èŠ‚

#### ç½‘ç»œç»“æž„ï¼ˆPolicyNetï¼‰ï¼š

\`\`\`text
self.network = torch.nn.Sequential(
    torch.nn.Linear(state_dim, hidden_dim),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_dim, hidden_dim),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_dim, action_dim)
)
\`\`\`

- ä½¿ç”¨ ä¸¤å±‚éšè—å±‚ã€ReLU æ¿€æ´»å‡½æ•°
- è¾“å‡ºé€šè¿‡ Softmax å¾—åˆ°åŠ¨ä½œæ¦‚çŽ‡åˆ†å¸ƒã€‚

#### ç­–ç•¥æ›´æ–°è¿‡ç¨‹ï¼š

\`\`\`python
log_prob = torch.log(self.policy_net(state).gather(1, action))
loss = -log_prob * G_tensor[i]
loss.backward(retain_graph=True)
\`\`\`

- ä½¿ç”¨ Monte Carlo å›žæŠ¥è®¡ç®— reward-to-goï¼›
- æŸå¤±å‡½æ•°æ˜¯è´Ÿçš„ log æ¦‚çŽ‡ä¹˜ä»¥æŠ˜æ‰£å¥–åŠ±ï¼Œç¬¦åˆç­–ç•¥æ¢¯åº¦çš„åŸºæœ¬å½¢å¼ã€‚

#### è®­ç»ƒæµç¨‹å›¾ï¼š

\`\`\`mermaid
flowchart TD
    A[æ”¶é›†çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±] --> B[è®¡ç®— reward-to-go Gt]
    B --> C[æ ‡å‡†åŒ– Gt]
    C --> D[é€çŠ¶æ€è®¡ç®— log_prob]
    D --> E[loss = -log_prob * Gt]
    E --> F[åå‘ä¼ æ’­ä¼˜åŒ–ç­–ç•¥ç½‘ç»œ]
\`\`\`

------



\`\`\`mermaid
flowchart TD
    B["for episode in 5000"] --> C["for step in 1000"]
    C --> D["agent take_action"]
    D --> A["env, state, action, reward, done"]
    A --> E{"done?"}
    E  -- å¦ --> C
    E  -- æ˜¯ --> G["agent updata"]
    G --> H{"av_reward >= 200"}
    H -- å¦ --> B
    G --> n1["Untitled Node"]

\`\`\`









### åˆ›æ–°ç‚¹

| é¡¹ç›®ç‰¹è‰²     | è¯´æ˜Ž                                                         |
| ------------ | ------------------------------------------------------------ |
| ç½‘ç»œç»“æž„è¾ƒæ·± | ç›¸æ¯”ä¼ ç»Ÿä¸€å±‚ç»“æž„ï¼Œå¼•å…¥ä¸¤å±‚éšè—å±‚ä½¿è¡¨è¾¾èƒ½åŠ›å¢žå¼ºï¼Œé€‚é…å¤æ‚ç­–ç•¥å­¦ä¹  |
| ä»£ç æ•´æ´æ˜“è¯» | ç»“æž„æ¸…æ™°ï¼Œä¾¿äºŽæ•™å­¦å±•ç¤ºå’ŒåŽç»­æ‰©å±•ï¼Œå¦‚å¼•å…¥ baselineã€actor-critic ç­‰ |
| è®­ç»ƒæµç¨‹æ¸…æ™° | reward-to-go + æ ‡å‡†åŒ–å›žæŠ¥ç»“åˆï¼Œæå‡å­¦ä¹ ç¨³å®šæ€§                |
| å¯æ‰©å±•æ€§å¼º   | å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ–¹ä¾¿åœ°å¼•å…¥ GAEã€ç†µæ­£åˆ™ç­‰æ”¹è¿›æ–¹æ³•               |

------

### é—®é¢˜åæ€

| é—®é¢˜           | åˆ†æž                                                         |
| -------------- | ------------------------------------------------------------ |
| æ˜¾å­˜å ç”¨å¤§     | æ¯æ­¥éƒ½ \`retain_graph=True\` ä¼šå¯¼è‡´å›¾ç»“æž„ä¿ç•™ï¼Œè®­ç»ƒé›†é•¿æˆ– batch å¤šæ—¶æ˜¾å­˜æš´æ¶¨ |
| æ”¶æ•›æ…¢         | çº¯ç­–ç•¥æ¢¯åº¦é«˜æ–¹å·®ï¼Œæ›´æ–°ä¸ç¨³å®šï¼Œlearning rate è®¾ç½®æ•æ„Ÿ         |
| æ—  base line   | æ²¡æœ‰ value function æˆ– baseline ä¼šå¯¼è‡´å­¦ä¹ æ•ˆçŽ‡è¾ƒä½Ž           |
| ä¸æ”¯æŒæ‰¹è®­ç»ƒ   | æ¯ä¸€ episode æ›´æ–°ä¸€æ¬¡ï¼Œä¸æ”¯æŒå¹¶è¡Œé‡‡æ ·ï¼Œæ•ˆçŽ‡ä½Žä¸‹              |
| ä¸å…·å¤‡æ³›åŒ–èƒ½åŠ› | è®­ç»ƒç»“æžœé«˜åº¦ä¾èµ–äºŽéšæœºé‡‡æ ·ï¼Œç­–ç•¥å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜             |

\`\`\`text
Episode 100	Average Score: -146.32
Episode 200	Average Score: -114.90
Episode 300	Average Score: -99.525
Episode 400	Average Score: -19.203
Episode 500	Average Score: 61.958
 â€¦â€¦â€¦â€¦
Episode 1800	Average Score: 186.97
Episode 1900	Average Score: 182.67
Episode 2000	Average Score: 171.60
Episode 2100	Average Score: 188.54
Episode 2170	Average Score: 200.12
\`\`\`







\`\`\`text
epoch = 1 --->  reward: -7.46
epoch = 2 --->  reward: 230.76
epoch = 3 --->  reward: 235.02
epoch = 4 --->  reward: 189.14
epoch = 5 --->  reward: 219.01
epoch = 6 --->  reward: 153.39
epoch = 7 --->  reward: 272.93
epoch = 8 --->  reward: 249.66
epoch = 9 --->  reward: 54.81
epoch = 10 --->  reward: 205.40
epoch = 11 --->  reward: 6.12
epoch = 12 --->  reward: 230.40
epoch = 13 --->  reward: 248.88
epoch = 14 --->  reward: 244.75
epoch = 15 --->  reward: 283.14
epoch = 16 --->  reward: -4.69
epoch = 17 --->  reward: 141.05
epoch = 18 --->  reward: 241.56
epoch = 19 --->  reward: 272.23
epoch = 20 --->  reward: 237.42
epoch = 21 --->  reward: 251.93
epoch = 22 --->  reward: 251.34
epoch = 23 --->  reward: 249.70
epoch = 24 --->  reward: 281.36
epoch = 25 --->  reward: 217.68
epoch = 26 --->  reward: 259.00
epoch = 27 --->  reward: 262.94
epoch = 28 --->  reward: 242.75
epoch = 29 --->  reward: 281.92
epoch = 30 --->  reward: 237.16
epoch = 31 --->  reward: 204.92
epoch = 32 --->  reward: 247.11
epoch = 33 --->  reward: 197.90
epoch = 34 --->  reward: 278.52
epoch = 35 --->  reward: 274.76
epoch = 36 --->  reward: 126.98
epoch = 37 --->  reward: 270.55
epoch = 38 --->  reward: 272.50
epoch = 39 --->  reward: 280.41
epoch = 40 --->  reward: 181.37
epoch = 41 --->  reward: 17.20
epoch = 42 --->  reward: 261.41
epoch = 43 --->  reward: 232.13
epoch = 44 --->  reward: 226.22
epoch = 45 --->  reward: 259.87
epoch = 46 --->  reward: 224.74
epoch = 47 --->  reward: 272.55
epoch = 48 --->  reward: 269.76
epoch = 49 --->  reward: 224.96
epoch = 50 --->  reward: 226.60
epoch = 51 --->  reward: 261.46
epoch = 52 --->  reward: 259.08
epoch = 53 --->  reward: 286.08
epoch = 54 --->  reward: 234.99
epoch = 55 --->  reward: 29.77
epoch = 56 --->  reward: 278.37
epoch = 57 --->  reward: 228.79
epoch = 58 --->  reward: 26.79
epoch = 59 --->  reward: 240.21
epoch = 60 --->  reward: -13.87
epoch = 61 --->  reward: 272.62
epoch = 62 --->  reward: 257.74
epoch = 63 --->  reward: -49.93
epoch = 64 --->  reward: 264.38
epoch = 65 --->  reward: 296.17
epoch = 66 --->  reward: 261.53
epoch = 67 --->  reward: -11.95
epoch = 68 --->  reward: 248.83
epoch = 69 --->  reward: 301.57
epoch = 70 --->  reward: 241.39
epoch = 71 --->  reward: 245.54
epoch = 72 --->  reward: 245.15
epoch = 73 --->  reward: 276.73
epoch = 74 --->  reward: 232.26
epoch = 75 --->  reward: 240.04
epoch = 76 --->  reward: 222.23
epoch = 77 --->  reward: 237.17
epoch = 78 --->  reward: 259.85
epoch = 79 --->  reward: -34.41
epoch = 80 --->  reward: 30.59
epoch = 81 --->  reward: 17.30
epoch = 82 --->  reward: 208.32
epoch = 83 --->  reward: 14.22
epoch = 84 --->  reward: 320.08
epoch = 85 --->  reward: 240.21
epoch = 86 --->  reward: -20.13
epoch = 87 --->  reward: 186.43
epoch = 88 --->  reward: 261.44
epoch = 89 --->  reward: 259.87
epoch = 90 --->  reward: 16.64
epoch = 91 --->  reward: 277.03
epoch = 92 --->  reward: 260.86
epoch = 93 --->  reward: 44.96
epoch = 94 --->  reward: 274.66
epoch = 95 --->  reward: 270.30
epoch = 96 --->  reward: 235.32
epoch = 97 --->  reward: 228.23
epoch = 98 --->  reward: 293.38
epoch = 99 --->  reward: 286.10
epoch = 100 --->  reward: 232.23



+-------------------+--------+
|      Metric       | Value  |
+-------------------+--------+
|  Average Reward   | 205.66 |
|   Success Rate    | 76.00% |
| Convergence Speed |  2070  |
|    Reward Std     | 95.69  |
+-------------------+--------+
\`\`\`









------

### ä¸€ã€å‚æ•°è¯´æ˜Ž

\`\`\`python
def update(self, transition_dict):
\`\`\`

- \`transition_dict\` æ˜¯ä¸€ä¸ªåŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
    - \`'rewards'\`ï¼šæ™ºèƒ½ä½“åœ¨æ¯ä¸€æ­¥èŽ·å¾—çš„å³æ—¶å¥–åŠ± \`r_t\`
    - \`'states'\`ï¼šæ¯ä¸€æ­¥å¯¹åº”çš„çŠ¶æ€ \`s_t\`
    - \`'actions'\`ï¼šæ¯ä¸€æ­¥æ™ºèƒ½ä½“é€‰æ‹©çš„åŠ¨ä½œ \`a_t\`

è¿™äº›æ˜¯ä¸€æ¬¡å®Œæ•´å›žåˆï¼ˆepisodeï¼‰çš„è½¨è¿¹æ•°æ®ã€‚

------

### äºŒã€è®¡ç®— reward-to-goï¼ˆå›žæŠ¥ï¼‰

\`\`\`python
G_list = []

for i in range(len(rewards)):
    
    G = 0
    discount = 1
    for j in range(i, len(rewards)):
        G += discount * rewards[j]
        discount *= self.gamma
    G_list.append(G)
\`\`\`

è¿™éƒ¨åˆ†æ˜¯è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„ ç´¯ç§¯å›žæŠ¥ï¼ˆReward-to-Goï¼‰ï¼š

- $G_t = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2} + ...$
- 
- G_list[i] å­˜çš„æ˜¯ä»Žç¬¬ \`i\` æ­¥å¼€å§‹ç´¯åŠ åˆ°ç»ˆç‚¹çš„æŠ˜æ‰£å›žæŠ¥ã€‚



è¿™ä¸€æ­¥çš„ä½œç”¨æ˜¯ä¸ºæ¯ä¸ªåŠ¨ä½œèµ‹äºˆä¸€ä¸ªâ€œä»·å€¼â€ï¼Œç”¨æ¥è¡¡é‡å…¶â€œå¥½åâ€ã€‚

------

### ä¸‰ã€å¥–åŠ±æ ‡å‡†åŒ–ï¼ˆç¨³å®šè®­ç»ƒï¼‰

\`\`\`python
G_tensor = torch.tensor(G_list, dtype=torch.float).to(self.device)
G_tensor = (G_tensor - G_tensor.mean()) / (G_tensor.std() + 1e-8)
\`\`\`

å°† \`G_list\` è½¬ä¸º tensorï¼Œå¹¶åšäº†æ ‡å‡†åŒ–å¤„ç†ï¼ˆå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼‰ï¼š



- ç›®çš„ï¼šé¿å…å› ä¸ºå¥–åŠ±å€¼èŒƒå›´å¤§ã€å˜åŒ–å‰§çƒˆï¼Œå¯¼è‡´ç½‘ç»œæ›´æ–°ä¸ç¨³å®šã€‚
- 1e-8 æ˜¯ä¸ºäº†é˜²æ­¢é™¤ä»¥ 0ã€‚

------

### å››ã€è®¡ç®—ç­–ç•¥æŸå¤±å¹¶åå‘ä¼ æ’­

\`\`\`python
for i in range(len(states)):
    
    state = torch.tensor([states[i]], dtype=torch.float).to(self.device)
    action = torch.tensor([actions[i]]).view(-1, 1).to(self.device)
    log_prob = torch.log(self.policy_net(state).gather(1, action))
    loss = -log_prob * G_tensor[i]
\`\`\`

- å¯¹æ¯ä¸€ä¸ªçŠ¶æ€ï¼š
    - è®¡ç®—å½“å‰ç­–ç•¥å¯¹ \`action[i]\` çš„æ¦‚çŽ‡
    - å–å¯¹æ•°æ¦‚çŽ‡ \`log_prob\`
    - æŸå¤±å‡½æ•°ï¼š\`-log_prob * G_tensor[i]\`

> \`âˆ‡J(Î¸) â‰ˆ âˆ‡ log Ï€_Î¸(a|s) * G_t\`









------

##  A2Cå¼ºåŒ–å­¦ä¹ æ¨¡åž‹å®žçŽ°æŠ¥å‘Š

### é€‰é¢˜æ„ä¹‰

| å†…å®¹é¡¹       | è¯´æ˜Ž                                                         |
| ------------ | ------------------------------------------------------------ |
| é¡¹ç›®èƒŒæ™¯ | åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼ŒREINFORCE å­˜åœ¨é«˜æ–¹å·®é—®é¢˜ï¼Œéš¾ä»¥ç¨³å®šå­¦ä¹ ã€‚Actor-Critic æ–¹æ³•é€šè¿‡å¼•å…¥â€œå€¼å‡½æ•°â€æ˜¾è‘—é™ä½Žæ–¹å·®ã€‚ |
| ç ”ç©¶åŠ¨æœº | é‡‡ç”¨ A2C æ–¹æ³•ï¼Œå¯ä»¥ç»“åˆç­–ç•¥ç½‘ç»œå’Œä»·å€¼ä¼°è®¡å™¨ï¼Œæå‡è®­ç»ƒæ•ˆçŽ‡ä¸Žç¨³å®šæ€§ï¼Œæ˜¯é€šå¾€ PPOã€TD3 ç­‰ä¸»æµç®—æ³•çš„åŸºç¡€ã€‚ |
| å®žç”¨ä»·å€¼ | é€‚ç”¨äºŽæ•™å­¦å®žéªŒã€æŽ§åˆ¶ç±»ä»»åŠ¡ã€OpenAI Gym è®­ç»ƒä»»åŠ¡ï¼Œå¦‚ CartPoleã€LunarLander ç­‰ã€‚ |
| è¯¾ç¨‹å…³è” | å¯¹åº”æœºå™¨å­¦ä¹ è¯¾ç¨‹ä¸­çš„ç­–ç•¥ä¼˜åŒ–éƒ¨åˆ†ï¼Œæ˜¯ç†è§£æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸»æµèŒƒå¼ï¼ˆPolicy Gradient + Baselineï¼‰çš„é‡è¦æ¡ˆä¾‹ã€‚ |

------

### æŠ€æœ¯ç»†èŠ‚

#### æ¨¡åž‹ç»“æž„ï¼ˆACnetï¼‰ï¼š

\`\`\`python
# çŠ¶æ€ -> ç‰¹å¾
self.affine = nn.Linear(state_dim, hidden_dim)
# ç‰¹å¾ -> ç­–ç•¥æ¦‚çŽ‡
self.action_layer = nn.Linear(hidden_dim, action_dim)
# ç‰¹å¾ -> çŠ¶æ€å€¼ä¼°è®¡
self.value_layer = nn.Linear(hidden_dim, 1)
\`\`\`

- ä½¿ç”¨å…±äº«ç‰¹å¾å±‚ï¼ˆ\`affine\`ï¼‰ï¼Œåˆ†åˆ«è¾“å‡ºåŠ¨ä½œæ¦‚çŽ‡å’ŒçŠ¶æ€å€¼ï¼›
- softmax ç”¨äºŽç”Ÿæˆ \`Categorical\` åˆ†å¸ƒå®žçŽ°åŠ¨ä½œé‡‡æ ·ã€‚

#### æŸå¤±å‡½æ•°è®¡ç®—ï¼š

\`\`\`python
advantages = discounted_rewards - state_values.detach()
policy_loss = -(logprobs * advantages).mean()
value_loss = F.smooth_l1_loss(state_values, discounted_rewards)
\`\`\`

- \`advantages\` æ˜¯å›žæŠ¥å‡åŽ» value baselineï¼›
- ç­–ç•¥æŸå¤±åŸºäºŽ Advantageï¼Œå€¼å‡½æ•°ä½¿ç”¨ Huber Lossï¼ˆæ›´ç¨³ï¼‰ï¼›
- æœ€ç»ˆæŸå¤±ä¸º \`policy_loss + 0.5 * value_loss\`ã€‚

#### æ›´æ–°é€»è¾‘ï¼ˆè®­ç»ƒï¼‰ï¼š

\`\`\`python
if episode % 20 == 0:
    loss = self.actor_critic.calculateLoss(self.gamma)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(...)  # æ¢¯åº¦è£å‰ª
\`\`\`

- æ¯ 20 ä¸ª episode æ›´æ–°ä¸€æ¬¡ï¼ˆè¾ƒä¿å®ˆï¼‰ï¼›
- ä½¿ç”¨æ¢¯åº¦è£å‰ªé¿å…çˆ†ç‚¸ï¼›
- ä½¿ç”¨ \`orthogonal_\` åˆå§‹åŒ–æå‡å­¦ä¹ è¡¨çŽ°ã€‚

------

### åˆ›æ–°ç‚¹

| é¡¹ç›®äº®ç‚¹               | æè¿°                                                         |
| ---------------------- | ------------------------------------------------------------ |
| Advantage ç”¨äºŽç­–ç•¥æ›´æ–° | ç›¸æ¯” REINFORCE ç›´æŽ¥ä¹˜ Gtï¼Œè¿™é‡Œç”¨äº† \`A = G - V\`ï¼Œæ–¹å·®æ˜¾è‘—ä¸‹é™ï¼Œè®­ç»ƒæ›´ç¨³ |
| å…±äº«ç‰¹å¾æå–å±‚         | å‡å°‘å†—ä½™è®¡ç®—ï¼Œç»“æž„ç´§å‡‘                                       |
| æ¢¯åº¦è£å‰ªä¸Žåˆå§‹åŒ–       | ä½¿ç”¨æ¢¯åº¦è£å‰ª + æ­£äº¤åˆå§‹åŒ–ï¼Œèƒ½æœ‰æ•ˆé˜²æ­¢ early divergence       |
| æ¨¡å—åŒ–ç»“æž„æ¸…æ™°         | ACnet + ActorCritic åˆ†å±‚æ˜Žç¡®ï¼ŒåŽç»­å¯æ‰©å±•ä¸º PPOã€GAE ç­‰å¤æ‚ç»“æž„ |

------

### é—®é¢˜åæ€

| é—®é¢˜           | åˆ†æž                                                         |
| -------------- | ------------------------------------------------------------ |
| æ›´æ–°é¢‘çŽ‡è¿‡ä½Ž   | æ¯ 20 ä¸ª episode æ‰æ›´æ–°ä¸€æ¬¡ï¼Œå¯èƒ½å¯¼è‡´å­¦ä¹ æ•ˆçŽ‡ä½Žä¸‹ï¼ˆå¯è°ƒå°ä¸ºæ¯ 5 æˆ– 10ï¼‰ |
| GPU åˆ©ç”¨çŽ‡ä½Ž   | æ— å¹¶è¡ŒçŽ¯å¢ƒé‡‡æ ·ï¼Œä¸èƒ½å‘æŒ¥ GPU å¹¶è¡Œä¼˜åŠ¿                        |
| åŠ¨ä½œç©ºé—´å—é™   | åªé€‚ç”¨äºŽç¦»æ•£åŠ¨ä½œï¼ˆ\`Categorical\`ï¼‰ï¼Œè‹¥çŽ¯å¢ƒä¸ºè¿žç»­åŠ¨ä½œéœ€æ”¹ä¸º \`Normal\` åˆ†å¸ƒ |
| æ— ç†µæ­£åˆ™       | æ²¡æœ‰é¼“åŠ±æŽ¢ç´¢é¡¹ï¼ˆå¦‚ \`entropy bonus\`ï¼‰ï¼Œå®¹æ˜“é™·å…¥ç¡®å®šæ€§ç­–ç•¥     |
| ä¼˜åŒ–å™¨å‚æ•°å›ºå®š | \`lr=0.02\` åå¤§ï¼Œå¯èƒ½ä¸é€‚ç”¨äºŽå¤æ‚çŽ¯å¢ƒï¼ˆå»ºè®®æ”¯æŒå¤–éƒ¨ä¼ å‚ï¼‰     |



- GAEï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰
- å¤šçº¿ç¨‹/å¼‚æ­¥æ›´æ–°ï¼ˆA3Cï¼‰
- åŠ¨ä½œç†µæ­£åˆ™é¡¹
- åŠ¨æ€å­¦ä¹ çŽ‡è°ƒæ•´



\`\`\`text
Episode 100	Average Score: -182.44	
Episode 200	Average Score: -45.25	
Episode 300	Average Score: 91.90	
Episode 400	Average Score: 73.00	
Episode 500	Average Score: 138.67	
Episode 600	Average Score: 149.06	
Episode 700	Average Score: 160.24	
Episode 800	Average Score: 183.50	
Episode 900	Average Score: 184.82	
Episode 1000	Average Score: 152.13	
Episode 1100	Average Score: 224.76	

Environment solved in 1000 episodes!
\`\`\`







\`\`\`text
epoch = 1 --->  reward: 246.66
epoch = 2 --->  reward: 17.74
epoch = 3 --->  reward: 191.91
epoch = 4 --->  reward: 271.08
epoch = 5 --->  reward: 305.89
epoch = 6 --->  reward: 206.65
epoch = 7 --->  reward: 259.00
epoch = 8 --->  reward: 264.70
epoch = 9 --->  reward: 172.43
epoch = 10 --->  reward: 259.59
epoch = 11 --->  reward: 201.08
epoch = 12 --->  reward: 243.69
epoch = 13 --->  reward: -0.12
epoch = 14 --->  reward: 199.81
epoch = 15 --->  reward: 36.01
epoch = 16 --->  reward: 246.89
epoch = 17 --->  reward: 230.58
epoch = 18 --->  reward: -20.30
epoch = 19 --->  reward: 209.98
epoch = 20 --->  reward: 252.83
epoch = 21 --->  reward: 129.04
epoch = 22 --->  reward: 264.21
epoch = 23 --->  reward: 241.42
epoch = 24 --->  reward: 306.58
epoch = 25 --->  reward: 278.17
epoch = 26 --->  reward: 17.92
epoch = 27 --->  reward: 210.00
epoch = 28 --->  reward: 273.66
epoch = 29 --->  reward: 253.30
epoch = 30 --->  reward: 262.71
epoch = 31 --->  reward: -47.90
epoch = 32 --->  reward: 26.72
epoch = 33 --->  reward: 242.17
epoch = 34 --->  reward: 191.67
epoch = 35 --->  reward: 192.02
epoch = 36 --->  reward: 263.20
epoch = 37 --->  reward: 253.00
epoch = 38 --->  reward: 38.76
epoch = 39 --->  reward: 222.76
epoch = 40 --->  reward: 246.02
epoch = 41 --->  reward: 251.06
epoch = 42 --->  reward: 190.10
epoch = 43 --->  reward: 257.36
epoch = 44 --->  reward: 271.18
epoch = 45 --->  reward: 181.01
epoch = 46 --->  reward: 233.50
epoch = 47 --->  reward: 262.74
epoch = 48 --->  reward: 138.02
epoch = 49 --->  reward: 225.49
epoch = 50 --->  reward: 28.23
epoch = 51 --->  reward: 268.10
epoch = 52 --->  reward: 199.24
epoch = 53 --->  reward: 199.27
epoch = 54 --->  reward: 205.86
epoch = 55 --->  reward: 256.46
epoch = 56 --->  reward: 221.23
epoch = 57 --->  reward: 248.23
epoch = 58 --->  reward: 273.67
epoch = 59 --->  reward: 290.61
epoch = 60 --->  reward: 203.94
epoch = 61 --->  reward: 240.32
epoch = 62 --->  reward: 225.91
epoch = 63 --->  reward: 166.63
epoch = 64 --->  reward: 266.55
epoch = 65 --->  reward: 264.40
epoch = 66 --->  reward: 250.21
epoch = 67 --->  reward: 245.96
epoch = 68 --->  reward: 250.84
epoch = 69 --->  reward: 255.69
epoch = 70 --->  reward: 302.20
epoch = 71 --->  reward: 160.22
epoch = 72 --->  reward: 290.81
epoch = 73 --->  reward: 191.10
epoch = 74 --->  reward: 275.69
epoch = 75 --->  reward: -1.92
epoch = 76 --->  reward: 241.92
epoch = 77 --->  reward: 218.00
epoch = 78 --->  reward: 220.85
epoch = 79 --->  reward: 232.89
epoch = 80 --->  reward: 244.90
epoch = 81 --->  reward: 279.01
epoch = 82 --->  reward: -6.69
epoch = 83 --->  reward: 251.86
epoch = 84 --->  reward: 190.44
epoch = 85 --->  reward: 236.91
epoch = 86 --->  reward: 175.14
epoch = 87 --->  reward: 236.48
epoch = 88 --->  reward: 255.99
epoch = 89 --->  reward: 127.52
epoch = 90 --->  reward: 268.69
epoch = 91 --->  reward: 187.20
epoch = 92 --->  reward: 280.32
epoch = 93 --->  reward: 260.04
epoch = 94 --->  reward: 237.37
epoch = 95 --->  reward: 266.44
epoch = 96 --->  reward: 270.32
epoch = 97 --->  reward: 132.23
epoch = 98 --->  reward: 250.84
epoch = 99 --->  reward: 264.41
epoch = 100 --->  reward: -5.84


+-------------------+-------------------+
|      Metric       |       Value       |
+-------------------+-------------------+
|  Average Reward   | 207.6866597495643 |
|   Success Rate    |      69.00%       |
| Convergence Speed |       1100        |
|    Reward Std     | 83.44128774131534 |
+-------------------+-------------------+
\`\`\`





\`\`\`mermaid
flowchart TD
    A[state] --> B[å…±äº«å±‚ state_dim --> hidden_dim]
    B --> C[Action \\nSoftmax+Categorical]
    B --> D[Value Head\\nLinear]
    C --> F[action å–æ ·]
    D --> E[state Value]
    
    E --> G[Store\\nactionæ¦‚çŽ‡å¯¹æ•° & state_values]
    F --> G
    G --> H[è®¡ç®—\\n policy loss + value loss]
    H --> I[åå‘ä¼ æ’­]

\`\`\`





\`\`\`mermaid
flowchart TD
	A[env]
	A --> |state| C[ACnet]
	A -->|Rewards| B[ActorCritic]

    C --> |Action| A
    B --> G[è®¡ç®— loss]
    G --> H[åå‘ä¼ æ’­]
    H --> I[æ¢¯åº¦è£å‰ª]
    I --> J[optim Step]
    J --> K[æ¸…ç©ºå­˜å‚¨çš„æ•°æ®]
	
\`\`\`



### 3. \`forward(self, state)\`
ç®—æ³•æµç¨‹ï¼š
1. è¾“å…¥å¤„ç†ï¼š
   - å°†è¾“å…¥ \`state\` è½¬æ¢ä¸ºæµ®ç‚¹å¼ é‡å¹¶ç§»è‡³
2. ç‰¹å¾æå–ï¼š
   - é€šè¿‡ \`affine\` å±‚ + ReLU æ¿€æ´»ï¼Œå¾—åˆ°éšè—ç‰¹å¾ \`features\`ã€‚
3. Criticï¼ˆä»·å€¼å¤´ï¼‰ï¼š
   - \`value_layer\` è¾“å‡ºçŠ¶æ€ä»·å€¼ \`state_value\`
4. Actorï¼ˆåŠ¨ä½œå¤´ï¼‰ï¼š
   - \`action_layer\` è¾“å‡ºåŠ¨ä½œ logits â†’ Softmax å½’ä¸€åŒ–ä¸ºæ¦‚çŽ‡ \`action_probs\`ã€‚
   - åˆ›å»ºåˆ†ç±»åˆ†å¸ƒ \`Categorical\`ï¼Œé‡‡æ ·åŠ¨ä½œ \`action\`ã€‚
5. è½¨è¿¹è®°å½•ï¼š
   - å­˜å‚¨ \`log_prob(action)\`ï¼ˆç­–ç•¥æ¢¯åº¦éœ€ç”¨ï¼‰å’Œ \`state_value\`ï¼ˆåŽç»­è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼‰ã€‚
6. è¿”å›žåŠ¨ä½œï¼š
   - è¿”å›žåŠ¨ä½œçš„æ ‡é‡å€¼ï¼ˆ\`.item()\` è„±ç¦»å­å›¾ï¼‰ã€‚

---

### 4. \`calculateLoss(self, gamma)\`
åŠŸèƒ½ï¼šè®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆActorï¼‰å’Œä»·å€¼æŸå¤±ï¼ˆCriticï¼‰ã€‚ 
ç®—æ³•æµç¨‹ï¼š

1. æ•°æ®å‡†å¤‡ï¼š
   - å°†ç¼“å†²åŒºä¸­çš„ \`rewards\`ã€\`logprobs\`ã€\`state_values\` è½¬ä¸ºå¼ é‡ã€‚
   - \`state_values\` åŽ»æŽ‰å¤šä½™çš„ç»´åº¦ï¼ˆ\`squeeze\`ï¼‰ã€‚
2. è®¡ç®—æŠ˜æ‰£å›žæŠ¥ï¼š
   - é€†åºéåŽ† \`rewards\`ï¼ŒæŒ‰å…¬å¼è®¡ç®—ç´¯ç§¯æŠ˜æ‰£å›žæŠ¥ï¼š 
   
   
   
   - $G_t = r_t + \\gamma \\cdot G_{t+1} $
   
   
   
   - ç»“æžœå­˜å…¥ \`discounted_rewards\`ã€‚
3. å›žæŠ¥æ ‡å‡†åŒ–ï¼š
   - å‡åŽ»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®ç¨³å®šè®­ç»ƒã€‚
4. ä¼˜åŠ¿å‡½æ•°è®¡ç®—ï¼š
   - $\\text{advantages} = \\text{discounted\\_rewards} - \\text{state\\_values}.detach() $
     ï¼ˆ\`detach()\` é˜»æ­¢æ¢¯åº¦æµå‘ Criticï¼Œä»…ç”¨äºŽè¯„ä¼°ç­–ç•¥ï¼‰ã€‚
5. æŸå¤±è®¡ç®—ï¼š
   - ç­–ç•¥æŸå¤±ï¼ˆActorï¼‰ï¼š 
     $ -\\mathbb{E}[\\log \\pi(a|s) \\cdot \\text{advantages}]$ 
   - ä»·å€¼æŸå¤±ï¼ˆCriticï¼‰ï¼š 
     å¹³æ»‘ L1 æŸå¤±ï¼ˆHuber æŸå¤±ï¼‰ï¼Œå‡å°‘ Critic çš„ä¼°å€¼è¯¯å·®ã€‚
6. æ€»æŸå¤±ï¼š
   - è¿”å›žåŠ æƒå’Œï¼š\`policy_loss + 0.5 * value_loss\`
   
   

1. Actor-Critic ååŒï¼š
   - Actorï¼šé€šè¿‡ \`logprobs\` å’Œä¼˜åŠ¿å‡½æ•°æ›´æ–°ç­–ç•¥ï¼Œå¼•å¯¼åŠ¨ä½œé€‰æ‹©ã€‚
   - Criticï¼šé€šè¿‡ \`state_values\` å’ŒæŠ˜æ‰£å›žæŠ¥çš„å·®å¼‚ä¼˜åŒ–ä»·å€¼ä¼°è®¡ã€‚
2. æŠ˜æ‰£å›žæŠ¥æ ‡å‡†åŒ–ï¼š
   - å‡å°‘ä¸åŒå›žåˆå›žæŠ¥çš„å°ºåº¦å·®å¼‚ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚
3. ä¼˜åŠ¿å‡½æ•°åˆ†ç¦»æ¢¯åº¦ï¼š
   - \`state_values.detach()\` ç¡®ä¿ Critic çš„æ¢¯åº¦ä¸å½±å“ Actor çš„æ›´æ–°ã€‚
4. æ­£äº¤åˆå§‹åŒ–ï¼š
   - ä¿æŒå‰å‘ä¼ æ’­ä¸­ä¿¡å·èŒƒæ•°ç¨³å®šï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ã€‚







| ç›®æ ‡å†…å®¹                            | å®žçŽ°æƒ…å†µè¯´æ˜Ž                                               |
|  ----------------------------------- | ---------------------------------------------------------- |
|  å®žçŽ°ç­–ç•¥æ¢¯åº¦ç®—æ³• | ä½¿ç”¨ REINFORCE æ–¹æ³•ï¼Œå«å¥–åŠ±æ ‡å‡†åŒ–ä¸Ž Monte Carlo å›žæŠ¥ä¼°è®¡ |
| å®žçŽ° Actor-Critic ç®—æ³•              | åŒ…å« actor-critic è”åˆç½‘ç»œï¼Œä½¿ç”¨ critic è¿›è¡Œä¼˜åŠ¿ä¼°è®¡     |
|æ”¯æŒè½¨è¿¹å­˜å‚¨ä¸ŽæŸå¤±è®¡ç®—              | å®žçŽ° log-probã€çŠ¶æ€ä»·å€¼ã€å¥–åŠ±çš„å­˜å‚¨ä¸Žå¤„ç†                |
|å¥–åŠ±æŠ˜æ‰£ä¸Žæ ‡å‡†åŒ–å¤„ç†                |  ä½¿ç”¨ gamma=0.99 è®¡ç®—æŠ˜æ‰£å›žæŠ¥å¹¶è¿›è¡Œæ ‡å‡†åŒ–                 |





### **2. Actor-Critic ç®—æ³•ï¼ˆ1é¡µPPTï¼‰**  
#### **æ ¸å¿ƒæ€æƒ³**  
è”åˆè®­ç»ƒç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰å’Œä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰ï¼Œç”¨Criticçš„ \\(V(s)\\) å‡å°æ–¹å·®ã€‚

---

#### **å…³é”®å…¬å¼**  
ä¼˜åŠ¿å‡½æ•°ï¼š 

$A(s,a) = R_t - V_\\phi(s)$



è”åˆæŸå¤±ï¼š 



$\\mathcal{L} = -\\log \\pi_\\theta(a|s) \\cdot A(s,a) + 0.5 \\cdot (V_\\phi(s) - R_t)^2$



å…¶ä¸­ï¼š  

- $\\pi_\\theta$æ˜¯Actorç½‘ç»œï¼Œ$V_\\phi$æ˜¯Criticç½‘ç»œ  
- $R_t$





\`\`\`mermaid
flowchart TD
    A[state] --> B[å…±äº«å±‚ state_dim --> hidden_dim]
    B --> C[action \\nhidden_dim --> action_dim]
    B --> D[critic\\nhidden_dim --> 1]

\`\`\`

`;export{n as default};
