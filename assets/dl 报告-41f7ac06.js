import{o as r,c as o,a as d}from"./index-7acf7975.js";const a={class:"markdown-body"},g="æŠ¥å‘Š",h="2025-06-01T00:00:00.000Z",i="2025-06-02T00:00:00.000Z",s="æ·±åº¦å­¦ä¹ ",w="å¼ºåŒ–å­¦ä¹ æŠ¥å‘Š",u={__name:"dl æŠ¥å‘Š",setup(n,{expose:t}){return t({frontmatter:{title:"æŠ¥å‘Š",date:"2025-06-01T00:00:00.000Z",updated:"2025-06-02T00:00:00.000Z",category:"æ·±åº¦å­¦ä¹ ",summary:"å¼ºåŒ–å­¦ä¹ æŠ¥å‘Š"}}),(l,e)=>(r(),o("div",a,e[0]||(e[0]=[d(`<hr><hr><h2>REINFORCE ç®—æ³•å®ç°æŠ¥å‘Š</h2><h3>é€‰é¢˜æ„ä¹‰</h3><table><thead><tr><th>å†…å®¹é¡¹</th><th>è¯´æ˜</th></tr></thead><tbody><tr><td>é¡¹ç›®èƒŒæ™¯</td><td>éšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œå¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰å·²åœ¨æ¸¸æˆæ™ºèƒ½ã€è‡ªåŠ¨æ§åˆ¶ã€æ¨èç³»ç»Ÿç­‰é¢†åŸŸå–å¾—çªç ´ã€‚</td></tr><tr><td>ç ”ç©¶åŠ¨æœº</td><td>REINFORCE æ˜¯æœ€åŸºç¡€çš„ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¹‹ä¸€ï¼Œå­¦ä¹ æˆæœ¬ä½ï¼Œé€‚åˆå…¥é—¨ç†è§£ç­–ç•¥ä¼˜åŒ–è¿‡ç¨‹ã€‚</td></tr><tr><td>å®ç”¨ä»·å€¼</td><td>é€šè¿‡è¯¥ç®—æ³•ï¼Œèƒ½ç›´è§‚ç†è§£â€œç­–ç•¥ç½‘ç»œ+é‡‡æ ·+å›æŠ¥è¯„ä¼°+ä¼˜åŒ–â€è¿™ä¸€é€šç”¨ RL æ¡†æ¶ï¼Œå…·å¤‡æ•™å­¦å’Œå®éªŒä»·å€¼ã€‚</td></tr></tbody></table><hr><h3>ğŸ›  æŠ€æœ¯ç»†èŠ‚</h3><h4>ç½‘ç»œç»“æ„ï¼ˆPolicyNetï¼‰ï¼š</h4><pre><code class="language-text">self.network = torch.nn.Sequential(
    torch.nn.Linear(state_dim, hidden_dim),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_dim, hidden_dim),
    torch.nn.ReLU(),
    torch.nn.Linear(hidden_dim, action_dim)
)
</code></pre><ul><li>ä½¿ç”¨ ä¸¤å±‚éšè—å±‚ã€ReLU æ¿€æ´»å‡½æ•°</li><li>è¾“å‡ºé€šè¿‡ Softmax å¾—åˆ°åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒã€‚</li></ul><h4>ç­–ç•¥æ›´æ–°è¿‡ç¨‹ï¼š</h4><pre><code class="language-python">log_prob = torch.log(self.policy_net(state).gather(1, action))
loss = -log_prob * G_tensor[i]
loss.backward(retain_graph=True)
</code></pre><ul><li>ä½¿ç”¨ Monte Carlo å›æŠ¥è®¡ç®— reward-to-goï¼›</li><li>æŸå¤±å‡½æ•°æ˜¯è´Ÿçš„ log æ¦‚ç‡ä¹˜ä»¥æŠ˜æ‰£å¥–åŠ±ï¼Œç¬¦åˆç­–ç•¥æ¢¯åº¦çš„åŸºæœ¬å½¢å¼ã€‚</li></ul><h4>è®­ç»ƒæµç¨‹å›¾ï¼š</h4><pre><code class="language-mermaid">flowchart TD
    A[æ”¶é›†çŠ¶æ€ã€åŠ¨ä½œã€å¥–åŠ±] --&gt; B[è®¡ç®— reward-to-go Gt]
    B --&gt; C[æ ‡å‡†åŒ– Gt]
    C --&gt; D[é€çŠ¶æ€è®¡ç®— log_prob]
    D --&gt; E[loss = -log_prob * Gt]
    E --&gt; F[åå‘ä¼ æ’­ä¼˜åŒ–ç­–ç•¥ç½‘ç»œ]
</code></pre><hr><pre><code class="language-mermaid">flowchart TD
    B[&quot;for episode in 5000&quot;] --&gt; C[&quot;for step in 1000&quot;]
    C --&gt; D[&quot;agent take_action&quot;]
    D --&gt; A[&quot;env, state, action, reward, done&quot;]
    A --&gt; E{&quot;done?&quot;}
    E  -- å¦ --&gt; C
    E  -- æ˜¯ --&gt; G[&quot;agent updata&quot;]
    G --&gt; H{&quot;av_reward &gt;= 200&quot;}
    H -- å¦ --&gt; B
    G --&gt; n1[&quot;Untitled Node&quot;]

</code></pre><h3>åˆ›æ–°ç‚¹</h3><table><thead><tr><th>é¡¹ç›®ç‰¹è‰²</th><th>è¯´æ˜</th></tr></thead><tbody><tr><td>ç½‘ç»œç»“æ„è¾ƒæ·±</td><td>ç›¸æ¯”ä¼ ç»Ÿä¸€å±‚ç»“æ„ï¼Œå¼•å…¥ä¸¤å±‚éšè—å±‚ä½¿è¡¨è¾¾èƒ½åŠ›å¢å¼ºï¼Œé€‚é…å¤æ‚ç­–ç•¥å­¦ä¹ </td></tr><tr><td>ä»£ç æ•´æ´æ˜“è¯»</td><td>ç»“æ„æ¸…æ™°ï¼Œä¾¿äºæ•™å­¦å±•ç¤ºå’Œåç»­æ‰©å±•ï¼Œå¦‚å¼•å…¥ baselineã€actor-critic ç­‰</td></tr><tr><td>è®­ç»ƒæµç¨‹æ¸…æ™°</td><td>reward-to-go + æ ‡å‡†åŒ–å›æŠ¥ç»“åˆï¼Œæå‡å­¦ä¹ ç¨³å®šæ€§</td></tr><tr><td>å¯æ‰©å±•æ€§å¼º</td><td>å¯ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ–¹ä¾¿åœ°å¼•å…¥ GAEã€ç†µæ­£åˆ™ç­‰æ”¹è¿›æ–¹æ³•</td></tr></tbody></table><hr><h3>é—®é¢˜åæ€</h3><table><thead><tr><th>é—®é¢˜</th><th>åˆ†æ</th></tr></thead><tbody><tr><td>æ˜¾å­˜å ç”¨å¤§</td><td>æ¯æ­¥éƒ½ <code>retain_graph=True</code> ä¼šå¯¼è‡´å›¾ç»“æ„ä¿ç•™ï¼Œè®­ç»ƒé›†é•¿æˆ– batch å¤šæ—¶æ˜¾å­˜æš´æ¶¨</td></tr><tr><td>æ”¶æ•›æ…¢</td><td>çº¯ç­–ç•¥æ¢¯åº¦é«˜æ–¹å·®ï¼Œæ›´æ–°ä¸ç¨³å®šï¼Œlearning rate è®¾ç½®æ•æ„Ÿ</td></tr><tr><td>æ—  base line</td><td>æ²¡æœ‰ value function æˆ– baseline ä¼šå¯¼è‡´å­¦ä¹ æ•ˆç‡è¾ƒä½</td></tr><tr><td>ä¸æ”¯æŒæ‰¹è®­ç»ƒ</td><td>æ¯ä¸€ episode æ›´æ–°ä¸€æ¬¡ï¼Œä¸æ”¯æŒå¹¶è¡Œé‡‡æ ·ï¼Œæ•ˆç‡ä½ä¸‹</td></tr><tr><td>ä¸å…·å¤‡æ³›åŒ–èƒ½åŠ›</td><td>è®­ç»ƒç»“æœé«˜åº¦ä¾èµ–äºéšæœºé‡‡æ ·ï¼Œç­–ç•¥å®¹æ˜“é™·å…¥å±€éƒ¨æœ€ä¼˜</td></tr></tbody></table><pre><code class="language-text">Episode 100	Average Score: -146.32
Episode 200	Average Score: -114.90
Episode 300	Average Score: -99.525
Episode 400	Average Score: -19.203
Episode 500	Average Score: 61.958
 â€¦â€¦â€¦â€¦
Episode 1800	Average Score: 186.97
Episode 1900	Average Score: 182.67
Episode 2000	Average Score: 171.60
Episode 2100	Average Score: 188.54
Episode 2170	Average Score: 200.12
</code></pre><pre><code class="language-text">epoch = 1 ---&gt;  reward: -7.46
epoch = 2 ---&gt;  reward: 230.76
epoch = 3 ---&gt;  reward: 235.02
epoch = 4 ---&gt;  reward: 189.14
epoch = 5 ---&gt;  reward: 219.01
epoch = 6 ---&gt;  reward: 153.39
epoch = 7 ---&gt;  reward: 272.93
epoch = 8 ---&gt;  reward: 249.66
epoch = 9 ---&gt;  reward: 54.81
epoch = 10 ---&gt;  reward: 205.40
epoch = 11 ---&gt;  reward: 6.12
epoch = 12 ---&gt;  reward: 230.40
epoch = 13 ---&gt;  reward: 248.88
epoch = 14 ---&gt;  reward: 244.75
epoch = 15 ---&gt;  reward: 283.14
epoch = 16 ---&gt;  reward: -4.69
epoch = 17 ---&gt;  reward: 141.05
epoch = 18 ---&gt;  reward: 241.56
epoch = 19 ---&gt;  reward: 272.23
epoch = 20 ---&gt;  reward: 237.42
epoch = 21 ---&gt;  reward: 251.93
epoch = 22 ---&gt;  reward: 251.34
epoch = 23 ---&gt;  reward: 249.70
epoch = 24 ---&gt;  reward: 281.36
epoch = 25 ---&gt;  reward: 217.68
epoch = 26 ---&gt;  reward: 259.00
epoch = 27 ---&gt;  reward: 262.94
epoch = 28 ---&gt;  reward: 242.75
epoch = 29 ---&gt;  reward: 281.92
epoch = 30 ---&gt;  reward: 237.16
epoch = 31 ---&gt;  reward: 204.92
epoch = 32 ---&gt;  reward: 247.11
epoch = 33 ---&gt;  reward: 197.90
epoch = 34 ---&gt;  reward: 278.52
epoch = 35 ---&gt;  reward: 274.76
epoch = 36 ---&gt;  reward: 126.98
epoch = 37 ---&gt;  reward: 270.55
epoch = 38 ---&gt;  reward: 272.50
epoch = 39 ---&gt;  reward: 280.41
epoch = 40 ---&gt;  reward: 181.37
epoch = 41 ---&gt;  reward: 17.20
epoch = 42 ---&gt;  reward: 261.41
epoch = 43 ---&gt;  reward: 232.13
epoch = 44 ---&gt;  reward: 226.22
epoch = 45 ---&gt;  reward: 259.87
epoch = 46 ---&gt;  reward: 224.74
epoch = 47 ---&gt;  reward: 272.55
epoch = 48 ---&gt;  reward: 269.76
epoch = 49 ---&gt;  reward: 224.96
epoch = 50 ---&gt;  reward: 226.60
epoch = 51 ---&gt;  reward: 261.46
epoch = 52 ---&gt;  reward: 259.08
epoch = 53 ---&gt;  reward: 286.08
epoch = 54 ---&gt;  reward: 234.99
epoch = 55 ---&gt;  reward: 29.77
epoch = 56 ---&gt;  reward: 278.37
epoch = 57 ---&gt;  reward: 228.79
epoch = 58 ---&gt;  reward: 26.79
epoch = 59 ---&gt;  reward: 240.21
epoch = 60 ---&gt;  reward: -13.87
epoch = 61 ---&gt;  reward: 272.62
epoch = 62 ---&gt;  reward: 257.74
epoch = 63 ---&gt;  reward: -49.93
epoch = 64 ---&gt;  reward: 264.38
epoch = 65 ---&gt;  reward: 296.17
epoch = 66 ---&gt;  reward: 261.53
epoch = 67 ---&gt;  reward: -11.95
epoch = 68 ---&gt;  reward: 248.83
epoch = 69 ---&gt;  reward: 301.57
epoch = 70 ---&gt;  reward: 241.39
epoch = 71 ---&gt;  reward: 245.54
epoch = 72 ---&gt;  reward: 245.15
epoch = 73 ---&gt;  reward: 276.73
epoch = 74 ---&gt;  reward: 232.26
epoch = 75 ---&gt;  reward: 240.04
epoch = 76 ---&gt;  reward: 222.23
epoch = 77 ---&gt;  reward: 237.17
epoch = 78 ---&gt;  reward: 259.85
epoch = 79 ---&gt;  reward: -34.41
epoch = 80 ---&gt;  reward: 30.59
epoch = 81 ---&gt;  reward: 17.30
epoch = 82 ---&gt;  reward: 208.32
epoch = 83 ---&gt;  reward: 14.22
epoch = 84 ---&gt;  reward: 320.08
epoch = 85 ---&gt;  reward: 240.21
epoch = 86 ---&gt;  reward: -20.13
epoch = 87 ---&gt;  reward: 186.43
epoch = 88 ---&gt;  reward: 261.44
epoch = 89 ---&gt;  reward: 259.87
epoch = 90 ---&gt;  reward: 16.64
epoch = 91 ---&gt;  reward: 277.03
epoch = 92 ---&gt;  reward: 260.86
epoch = 93 ---&gt;  reward: 44.96
epoch = 94 ---&gt;  reward: 274.66
epoch = 95 ---&gt;  reward: 270.30
epoch = 96 ---&gt;  reward: 235.32
epoch = 97 ---&gt;  reward: 228.23
epoch = 98 ---&gt;  reward: 293.38
epoch = 99 ---&gt;  reward: 286.10
epoch = 100 ---&gt;  reward: 232.23



+-------------------+--------+
|      Metric       | Value  |
+-------------------+--------+
|  Average Reward   | 205.66 |
|   Success Rate    | 76.00% |
| Convergence Speed |  2070  |
|    Reward Std     | 95.69  |
+-------------------+--------+
</code></pre><hr><h3>ä¸€ã€å‚æ•°è¯´æ˜</h3><pre><code class="language-python">def update(self, transition_dict):
</code></pre><ul><li><code>transition_dict</code> æ˜¯ä¸€ä¸ªåŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š <ul><li><code>&#39;rewards&#39;</code>ï¼šæ™ºèƒ½ä½“åœ¨æ¯ä¸€æ­¥è·å¾—çš„å³æ—¶å¥–åŠ± <code>r_t</code></li><li><code>&#39;states&#39;</code>ï¼šæ¯ä¸€æ­¥å¯¹åº”çš„çŠ¶æ€ <code>s_t</code></li><li><code>&#39;actions&#39;</code>ï¼šæ¯ä¸€æ­¥æ™ºèƒ½ä½“é€‰æ‹©çš„åŠ¨ä½œ <code>a_t</code></li></ul></li></ul><p>è¿™äº›æ˜¯ä¸€æ¬¡å®Œæ•´å›åˆï¼ˆepisodeï¼‰çš„è½¨è¿¹æ•°æ®ã€‚</p><hr><h3>äºŒã€è®¡ç®— reward-to-goï¼ˆå›æŠ¥ï¼‰</h3><pre><code class="language-python">G_list = []

for i in range(len(rewards)):
    
    G = 0
    discount = 1
    for j in range(i, len(rewards)):
        G += discount * rewards[j]
        discount *= self.gamma
    G_list.append(G)
</code></pre><p>è¿™éƒ¨åˆ†æ˜¯è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„ ç´¯ç§¯å›æŠ¥ï¼ˆReward-to-Goï¼‰ï¼š</p><ul><li>$G_t = r_t + Î³Â·r_{t+1} + Î³Â²Â·r_{t+2} + â€¦$</li><li></li><li>G_list[i] å­˜çš„æ˜¯ä»ç¬¬ <code>i</code> æ­¥å¼€å§‹ç´¯åŠ åˆ°ç»ˆç‚¹çš„æŠ˜æ‰£å›æŠ¥ã€‚</li></ul><p>è¿™ä¸€æ­¥çš„ä½œç”¨æ˜¯ä¸ºæ¯ä¸ªåŠ¨ä½œèµ‹äºˆä¸€ä¸ªâ€œä»·å€¼â€ï¼Œç”¨æ¥è¡¡é‡å…¶â€œå¥½åâ€ã€‚</p><hr><h3>ä¸‰ã€å¥–åŠ±æ ‡å‡†åŒ–ï¼ˆç¨³å®šè®­ç»ƒï¼‰</h3><pre><code class="language-python">G_tensor = torch.tensor(G_list, dtype=torch.float).to(self.device)
G_tensor = (G_tensor - G_tensor.mean()) / (G_tensor.std() + 1e-8)
</code></pre><p>å°† <code>G_list</code> è½¬ä¸º tensorï¼Œå¹¶åšäº†æ ‡å‡†åŒ–å¤„ç†ï¼ˆå‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 1ï¼‰ï¼š</p><ul><li>ç›®çš„ï¼šé¿å…å› ä¸ºå¥–åŠ±å€¼èŒƒå›´å¤§ã€å˜åŒ–å‰§çƒˆï¼Œå¯¼è‡´ç½‘ç»œæ›´æ–°ä¸ç¨³å®šã€‚</li><li>1e-8 æ˜¯ä¸ºäº†é˜²æ­¢é™¤ä»¥ 0ã€‚</li></ul><hr><h3>å››ã€è®¡ç®—ç­–ç•¥æŸå¤±å¹¶åå‘ä¼ æ’­</h3><pre><code class="language-python">for i in range(len(states)):
    
    state = torch.tensor([states[i]], dtype=torch.float).to(self.device)
    action = torch.tensor([actions[i]]).view(-1, 1).to(self.device)
    log_prob = torch.log(self.policy_net(state).gather(1, action))
    loss = -log_prob * G_tensor[i]
</code></pre><ul><li>å¯¹æ¯ä¸€ä¸ªçŠ¶æ€ï¼š <ul><li>è®¡ç®—å½“å‰ç­–ç•¥å¯¹ <code>action[i]</code> çš„æ¦‚ç‡</li><li>å–å¯¹æ•°æ¦‚ç‡ <code>log_prob</code></li><li>æŸå¤±å‡½æ•°ï¼š<code>-log_prob * G_tensor[i]</code></li></ul></li></ul><blockquote><p><code>âˆ‡J(Î¸) â‰ˆ âˆ‡ log Ï€_Î¸(a|s) * G_t</code></p></blockquote><hr><h2>A2Cå¼ºåŒ–å­¦ä¹ æ¨¡å‹å®ç°æŠ¥å‘Š</h2><h3>é€‰é¢˜æ„ä¹‰</h3><table><thead><tr><th>å†…å®¹é¡¹</th><th>è¯´æ˜</th></tr></thead><tbody><tr><td>é¡¹ç›®èƒŒæ™¯</td><td>åœ¨ç­–ç•¥æ¢¯åº¦æ–¹æ³•ä¸­ï¼ŒREINFORCE å­˜åœ¨é«˜æ–¹å·®é—®é¢˜ï¼Œéš¾ä»¥ç¨³å®šå­¦ä¹ ã€‚Actor-Critic æ–¹æ³•é€šè¿‡å¼•å…¥â€œå€¼å‡½æ•°â€æ˜¾è‘—é™ä½æ–¹å·®ã€‚</td></tr><tr><td>ç ”ç©¶åŠ¨æœº</td><td>é‡‡ç”¨ A2C æ–¹æ³•ï¼Œå¯ä»¥ç»“åˆç­–ç•¥ç½‘ç»œå’Œä»·å€¼ä¼°è®¡å™¨ï¼Œæå‡è®­ç»ƒæ•ˆç‡ä¸ç¨³å®šæ€§ï¼Œæ˜¯é€šå¾€ PPOã€TD3 ç­‰ä¸»æµç®—æ³•çš„åŸºç¡€ã€‚</td></tr><tr><td>å®ç”¨ä»·å€¼</td><td>é€‚ç”¨äºæ•™å­¦å®éªŒã€æ§åˆ¶ç±»ä»»åŠ¡ã€OpenAI Gym è®­ç»ƒä»»åŠ¡ï¼Œå¦‚ CartPoleã€LunarLander ç­‰ã€‚</td></tr><tr><td>è¯¾ç¨‹å…³è”</td><td>å¯¹åº”æœºå™¨å­¦ä¹ è¯¾ç¨‹ä¸­çš„ç­–ç•¥ä¼˜åŒ–éƒ¨åˆ†ï¼Œæ˜¯ç†è§£æ·±åº¦å¼ºåŒ–å­¦ä¹ ä¸»æµèŒƒå¼ï¼ˆPolicy Gradient + Baselineï¼‰çš„é‡è¦æ¡ˆä¾‹ã€‚</td></tr></tbody></table><hr><h3>æŠ€æœ¯ç»†èŠ‚</h3><h4>æ¨¡å‹ç»“æ„ï¼ˆACnetï¼‰ï¼š</h4><pre><code class="language-python"># çŠ¶æ€ -&gt; ç‰¹å¾
self.affine = nn.Linear(state_dim, hidden_dim)
# ç‰¹å¾ -&gt; ç­–ç•¥æ¦‚ç‡
self.action_layer = nn.Linear(hidden_dim, action_dim)
# ç‰¹å¾ -&gt; çŠ¶æ€å€¼ä¼°è®¡
self.value_layer = nn.Linear(hidden_dim, 1)
</code></pre><ul><li>ä½¿ç”¨å…±äº«ç‰¹å¾å±‚ï¼ˆ<code>affine</code>ï¼‰ï¼Œåˆ†åˆ«è¾“å‡ºåŠ¨ä½œæ¦‚ç‡å’ŒçŠ¶æ€å€¼ï¼›</li><li>softmax ç”¨äºç”Ÿæˆ <code>Categorical</code> åˆ†å¸ƒå®ç°åŠ¨ä½œé‡‡æ ·ã€‚</li></ul><h4>æŸå¤±å‡½æ•°è®¡ç®—ï¼š</h4><pre><code class="language-python">advantages = discounted_rewards - state_values.detach()
policy_loss = -(logprobs * advantages).mean()
value_loss = F.smooth_l1_loss(state_values, discounted_rewards)
</code></pre><ul><li><code>advantages</code> æ˜¯å›æŠ¥å‡å» value baselineï¼›</li><li>ç­–ç•¥æŸå¤±åŸºäº Advantageï¼Œå€¼å‡½æ•°ä½¿ç”¨ Huber Lossï¼ˆæ›´ç¨³ï¼‰ï¼›</li><li>æœ€ç»ˆæŸå¤±ä¸º <code>policy_loss + 0.5 * value_loss</code>ã€‚</li></ul><h4>æ›´æ–°é€»è¾‘ï¼ˆè®­ç»ƒï¼‰ï¼š</h4><pre><code class="language-python">if episode % 20 == 0:
    loss = self.actor_critic.calculateLoss(self.gamma)
    loss.backward()
    torch.nn.utils.clip_grad_norm_(...)  # æ¢¯åº¦è£å‰ª
</code></pre><ul><li>æ¯ 20 ä¸ª episode æ›´æ–°ä¸€æ¬¡ï¼ˆè¾ƒä¿å®ˆï¼‰ï¼›</li><li>ä½¿ç”¨æ¢¯åº¦è£å‰ªé¿å…çˆ†ç‚¸ï¼›</li><li>ä½¿ç”¨ <code>orthogonal_</code> åˆå§‹åŒ–æå‡å­¦ä¹ è¡¨ç°ã€‚</li></ul><hr><h3>åˆ›æ–°ç‚¹</h3><table><thead><tr><th>é¡¹ç›®äº®ç‚¹</th><th>æè¿°</th></tr></thead><tbody><tr><td>Advantage ç”¨äºç­–ç•¥æ›´æ–°</td><td>ç›¸æ¯” REINFORCE ç›´æ¥ä¹˜ Gtï¼Œè¿™é‡Œç”¨äº† <code>A = G - V</code>ï¼Œæ–¹å·®æ˜¾è‘—ä¸‹é™ï¼Œè®­ç»ƒæ›´ç¨³</td></tr><tr><td>å…±äº«ç‰¹å¾æå–å±‚</td><td>å‡å°‘å†—ä½™è®¡ç®—ï¼Œç»“æ„ç´§å‡‘</td></tr><tr><td>æ¢¯åº¦è£å‰ªä¸åˆå§‹åŒ–</td><td>ä½¿ç”¨æ¢¯åº¦è£å‰ª + æ­£äº¤åˆå§‹åŒ–ï¼Œèƒ½æœ‰æ•ˆé˜²æ­¢ early divergence</td></tr><tr><td>æ¨¡å—åŒ–ç»“æ„æ¸…æ™°</td><td>ACnet + ActorCritic åˆ†å±‚æ˜ç¡®ï¼Œåç»­å¯æ‰©å±•ä¸º PPOã€GAE ç­‰å¤æ‚ç»“æ„</td></tr></tbody></table><hr><h3>é—®é¢˜åæ€</h3><table><thead><tr><th>é—®é¢˜</th><th>åˆ†æ</th></tr></thead><tbody><tr><td>æ›´æ–°é¢‘ç‡è¿‡ä½</td><td>æ¯ 20 ä¸ª episode æ‰æ›´æ–°ä¸€æ¬¡ï¼Œå¯èƒ½å¯¼è‡´å­¦ä¹ æ•ˆç‡ä½ä¸‹ï¼ˆå¯è°ƒå°ä¸ºæ¯ 5 æˆ– 10ï¼‰</td></tr><tr><td>GPU åˆ©ç”¨ç‡ä½</td><td>æ— å¹¶è¡Œç¯å¢ƒé‡‡æ ·ï¼Œä¸èƒ½å‘æŒ¥ GPU å¹¶è¡Œä¼˜åŠ¿</td></tr><tr><td>åŠ¨ä½œç©ºé—´å—é™</td><td>åªé€‚ç”¨äºç¦»æ•£åŠ¨ä½œï¼ˆ<code>Categorical</code>ï¼‰ï¼Œè‹¥ç¯å¢ƒä¸ºè¿ç»­åŠ¨ä½œéœ€æ”¹ä¸º <code>Normal</code> åˆ†å¸ƒ</td></tr><tr><td>æ— ç†µæ­£åˆ™</td><td>æ²¡æœ‰é¼“åŠ±æ¢ç´¢é¡¹ï¼ˆå¦‚ <code>entropy bonus</code>ï¼‰ï¼Œå®¹æ˜“é™·å…¥ç¡®å®šæ€§ç­–ç•¥</td></tr><tr><td>ä¼˜åŒ–å™¨å‚æ•°å›ºå®š</td><td><code>lr=0.02</code> åå¤§ï¼Œå¯èƒ½ä¸é€‚ç”¨äºå¤æ‚ç¯å¢ƒï¼ˆå»ºè®®æ”¯æŒå¤–éƒ¨ä¼ å‚ï¼‰</td></tr></tbody></table><ul><li>GAEï¼ˆå¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡ï¼‰</li><li>å¤šçº¿ç¨‹/å¼‚æ­¥æ›´æ–°ï¼ˆA3Cï¼‰</li><li>åŠ¨ä½œç†µæ­£åˆ™é¡¹</li><li>åŠ¨æ€å­¦ä¹ ç‡è°ƒæ•´</li></ul><pre><code class="language-text">Episode 100	Average Score: -182.44	
Episode 200	Average Score: -45.25	
Episode 300	Average Score: 91.90	
Episode 400	Average Score: 73.00	
Episode 500	Average Score: 138.67	
Episode 600	Average Score: 149.06	
Episode 700	Average Score: 160.24	
Episode 800	Average Score: 183.50	
Episode 900	Average Score: 184.82	
Episode 1000	Average Score: 152.13	
Episode 1100	Average Score: 224.76	

Environment solved in 1000 episodes!
</code></pre><pre><code class="language-text">epoch = 1 ---&gt;  reward: 246.66
epoch = 2 ---&gt;  reward: 17.74
epoch = 3 ---&gt;  reward: 191.91
epoch = 4 ---&gt;  reward: 271.08
epoch = 5 ---&gt;  reward: 305.89
epoch = 6 ---&gt;  reward: 206.65
epoch = 7 ---&gt;  reward: 259.00
epoch = 8 ---&gt;  reward: 264.70
epoch = 9 ---&gt;  reward: 172.43
epoch = 10 ---&gt;  reward: 259.59
epoch = 11 ---&gt;  reward: 201.08
epoch = 12 ---&gt;  reward: 243.69
epoch = 13 ---&gt;  reward: -0.12
epoch = 14 ---&gt;  reward: 199.81
epoch = 15 ---&gt;  reward: 36.01
epoch = 16 ---&gt;  reward: 246.89
epoch = 17 ---&gt;  reward: 230.58
epoch = 18 ---&gt;  reward: -20.30
epoch = 19 ---&gt;  reward: 209.98
epoch = 20 ---&gt;  reward: 252.83
epoch = 21 ---&gt;  reward: 129.04
epoch = 22 ---&gt;  reward: 264.21
epoch = 23 ---&gt;  reward: 241.42
epoch = 24 ---&gt;  reward: 306.58
epoch = 25 ---&gt;  reward: 278.17
epoch = 26 ---&gt;  reward: 17.92
epoch = 27 ---&gt;  reward: 210.00
epoch = 28 ---&gt;  reward: 273.66
epoch = 29 ---&gt;  reward: 253.30
epoch = 30 ---&gt;  reward: 262.71
epoch = 31 ---&gt;  reward: -47.90
epoch = 32 ---&gt;  reward: 26.72
epoch = 33 ---&gt;  reward: 242.17
epoch = 34 ---&gt;  reward: 191.67
epoch = 35 ---&gt;  reward: 192.02
epoch = 36 ---&gt;  reward: 263.20
epoch = 37 ---&gt;  reward: 253.00
epoch = 38 ---&gt;  reward: 38.76
epoch = 39 ---&gt;  reward: 222.76
epoch = 40 ---&gt;  reward: 246.02
epoch = 41 ---&gt;  reward: 251.06
epoch = 42 ---&gt;  reward: 190.10
epoch = 43 ---&gt;  reward: 257.36
epoch = 44 ---&gt;  reward: 271.18
epoch = 45 ---&gt;  reward: 181.01
epoch = 46 ---&gt;  reward: 233.50
epoch = 47 ---&gt;  reward: 262.74
epoch = 48 ---&gt;  reward: 138.02
epoch = 49 ---&gt;  reward: 225.49
epoch = 50 ---&gt;  reward: 28.23
epoch = 51 ---&gt;  reward: 268.10
epoch = 52 ---&gt;  reward: 199.24
epoch = 53 ---&gt;  reward: 199.27
epoch = 54 ---&gt;  reward: 205.86
epoch = 55 ---&gt;  reward: 256.46
epoch = 56 ---&gt;  reward: 221.23
epoch = 57 ---&gt;  reward: 248.23
epoch = 58 ---&gt;  reward: 273.67
epoch = 59 ---&gt;  reward: 290.61
epoch = 60 ---&gt;  reward: 203.94
epoch = 61 ---&gt;  reward: 240.32
epoch = 62 ---&gt;  reward: 225.91
epoch = 63 ---&gt;  reward: 166.63
epoch = 64 ---&gt;  reward: 266.55
epoch = 65 ---&gt;  reward: 264.40
epoch = 66 ---&gt;  reward: 250.21
epoch = 67 ---&gt;  reward: 245.96
epoch = 68 ---&gt;  reward: 250.84
epoch = 69 ---&gt;  reward: 255.69
epoch = 70 ---&gt;  reward: 302.20
epoch = 71 ---&gt;  reward: 160.22
epoch = 72 ---&gt;  reward: 290.81
epoch = 73 ---&gt;  reward: 191.10
epoch = 74 ---&gt;  reward: 275.69
epoch = 75 ---&gt;  reward: -1.92
epoch = 76 ---&gt;  reward: 241.92
epoch = 77 ---&gt;  reward: 218.00
epoch = 78 ---&gt;  reward: 220.85
epoch = 79 ---&gt;  reward: 232.89
epoch = 80 ---&gt;  reward: 244.90
epoch = 81 ---&gt;  reward: 279.01
epoch = 82 ---&gt;  reward: -6.69
epoch = 83 ---&gt;  reward: 251.86
epoch = 84 ---&gt;  reward: 190.44
epoch = 85 ---&gt;  reward: 236.91
epoch = 86 ---&gt;  reward: 175.14
epoch = 87 ---&gt;  reward: 236.48
epoch = 88 ---&gt;  reward: 255.99
epoch = 89 ---&gt;  reward: 127.52
epoch = 90 ---&gt;  reward: 268.69
epoch = 91 ---&gt;  reward: 187.20
epoch = 92 ---&gt;  reward: 280.32
epoch = 93 ---&gt;  reward: 260.04
epoch = 94 ---&gt;  reward: 237.37
epoch = 95 ---&gt;  reward: 266.44
epoch = 96 ---&gt;  reward: 270.32
epoch = 97 ---&gt;  reward: 132.23
epoch = 98 ---&gt;  reward: 250.84
epoch = 99 ---&gt;  reward: 264.41
epoch = 100 ---&gt;  reward: -5.84


+-------------------+-------------------+
|      Metric       |       Value       |
+-------------------+-------------------+
|  Average Reward   | 207.6866597495643 |
|   Success Rate    |      69.00%       |
| Convergence Speed |       1100        |
|    Reward Std     | 83.44128774131534 |
+-------------------+-------------------+
</code></pre><pre><code class="language-mermaid">flowchart TD
    A[state] --&gt; B[å…±äº«å±‚ state_dim --&gt; hidden_dim]
    B --&gt; C[Action \\nSoftmax+Categorical]
    B --&gt; D[Value Head\\nLinear]
    C --&gt; F[action å–æ ·]
    D --&gt; E[state Value]
    
    E --&gt; G[Store\\nactionæ¦‚ç‡å¯¹æ•° &amp; state_values]
    F --&gt; G
    G --&gt; H[è®¡ç®—\\n policy loss + value loss]
    H --&gt; I[åå‘ä¼ æ’­]

</code></pre><pre><code class="language-mermaid">flowchart TD
	A[env]
	A --&gt; |state| C[ACnet]
	A --&gt;|Rewards| B[ActorCritic]

    C --&gt; |Action| A
    B --&gt; G[è®¡ç®— loss]
    G --&gt; H[åå‘ä¼ æ’­]
    H --&gt; I[æ¢¯åº¦è£å‰ª]
    I --&gt; J[optim Step]
    J --&gt; K[æ¸…ç©ºå­˜å‚¨çš„æ•°æ®]
	
</code></pre><h3>3. <code>forward(self, state)</code></h3><p>ç®—æ³•æµç¨‹ï¼š</p><ol><li>è¾“å…¥å¤„ç†ï¼š <ul><li>å°†è¾“å…¥ <code>state</code> è½¬æ¢ä¸ºæµ®ç‚¹å¼ é‡å¹¶ç§»è‡³</li></ul></li><li>ç‰¹å¾æå–ï¼š <ul><li>é€šè¿‡ <code>affine</code> å±‚ + ReLU æ¿€æ´»ï¼Œå¾—åˆ°éšè—ç‰¹å¾ <code>features</code>ã€‚</li></ul></li><li>Criticï¼ˆä»·å€¼å¤´ï¼‰ï¼š <ul><li><code>value_layer</code> è¾“å‡ºçŠ¶æ€ä»·å€¼ <code>state_value</code></li></ul></li><li>Actorï¼ˆåŠ¨ä½œå¤´ï¼‰ï¼š <ul><li><code>action_layer</code> è¾“å‡ºåŠ¨ä½œ logits â†’ Softmax å½’ä¸€åŒ–ä¸ºæ¦‚ç‡ <code>action_probs</code>ã€‚</li><li>åˆ›å»ºåˆ†ç±»åˆ†å¸ƒ <code>Categorical</code>ï¼Œé‡‡æ ·åŠ¨ä½œ <code>action</code>ã€‚</li></ul></li><li>è½¨è¿¹è®°å½•ï¼š <ul><li>å­˜å‚¨ <code>log_prob(action)</code>ï¼ˆç­–ç•¥æ¢¯åº¦éœ€ç”¨ï¼‰å’Œ <code>state_value</code>ï¼ˆåç»­è®¡ç®—ä¼˜åŠ¿å‡½æ•°ï¼‰ã€‚</li></ul></li><li>è¿”å›åŠ¨ä½œï¼š <ul><li>è¿”å›åŠ¨ä½œçš„æ ‡é‡å€¼ï¼ˆ<code>.item()</code> è„±ç¦»å­å›¾ï¼‰ã€‚</li></ul></li></ol><hr><h3>4. <code>calculateLoss(self, gamma)</code></h3><p>åŠŸèƒ½ï¼šè®¡ç®—ç­–ç•¥æŸå¤±ï¼ˆActorï¼‰å’Œä»·å€¼æŸå¤±ï¼ˆCriticï¼‰ã€‚ ç®—æ³•æµç¨‹ï¼š</p><ol><li><p>æ•°æ®å‡†å¤‡ï¼š</p><ul><li>å°†ç¼“å†²åŒºä¸­çš„ <code>rewards</code>ã€<code>logprobs</code>ã€<code>state_values</code> è½¬ä¸ºå¼ é‡ã€‚</li><li><code>state_values</code> å»æ‰å¤šä½™çš„ç»´åº¦ï¼ˆ<code>squeeze</code>ï¼‰ã€‚</li></ul></li><li><p>è®¡ç®—æŠ˜æ‰£å›æŠ¥ï¼š</p><ul><li><p>é€†åºéå† <code>rewards</code>ï¼ŒæŒ‰å…¬å¼è®¡ç®—ç´¯ç§¯æŠ˜æ‰£å›æŠ¥ï¼š</p></li><li><p>$G_t = r_t + \\gamma \\cdot G_{t+1} $</p></li><li><p>ç»“æœå­˜å…¥ <code>discounted_rewards</code>ã€‚</p></li></ul></li><li><p>å›æŠ¥æ ‡å‡†åŒ–ï¼š</p><ul><li>å‡å»å‡å€¼ï¼Œé™¤ä»¥æ ‡å‡†å·®ç¨³å®šè®­ç»ƒã€‚</li></ul></li><li><p>ä¼˜åŠ¿å‡½æ•°è®¡ç®—ï¼š</p><ul><li>$\\text{advantages} = \\text{discounted_rewards} - \\text{state_values}.detach() $ ï¼ˆ<code>detach()</code> é˜»æ­¢æ¢¯åº¦æµå‘ Criticï¼Œä»…ç”¨äºè¯„ä¼°ç­–ç•¥ï¼‰ã€‚</li></ul></li><li><p>æŸå¤±è®¡ç®—ï¼š</p><ul><li>ç­–ç•¥æŸå¤±ï¼ˆActorï¼‰ï¼š $ -\\mathbb{E}[\\log \\pi(a|s) \\cdot \\text{advantages}]$</li><li>ä»·å€¼æŸå¤±ï¼ˆCriticï¼‰ï¼š å¹³æ»‘ L1 æŸå¤±ï¼ˆHuber æŸå¤±ï¼‰ï¼Œå‡å°‘ Critic çš„ä¼°å€¼è¯¯å·®ã€‚</li></ul></li><li><p>æ€»æŸå¤±ï¼š</p><ul><li>è¿”å›åŠ æƒå’Œï¼š<code>policy_loss + 0.5 * value_loss</code></li></ul></li><li><p>Actor-Critic ååŒï¼š</p><ul><li>Actorï¼šé€šè¿‡ <code>logprobs</code> å’Œä¼˜åŠ¿å‡½æ•°æ›´æ–°ç­–ç•¥ï¼Œå¼•å¯¼åŠ¨ä½œé€‰æ‹©ã€‚</li><li>Criticï¼šé€šè¿‡ <code>state_values</code> å’ŒæŠ˜æ‰£å›æŠ¥çš„å·®å¼‚ä¼˜åŒ–ä»·å€¼ä¼°è®¡ã€‚</li></ul></li><li><p>æŠ˜æ‰£å›æŠ¥æ ‡å‡†åŒ–ï¼š</p><ul><li>å‡å°‘ä¸åŒå›åˆå›æŠ¥çš„å°ºåº¦å·®å¼‚ï¼ŒåŠ é€Ÿæ”¶æ•›ã€‚</li></ul></li><li><p>ä¼˜åŠ¿å‡½æ•°åˆ†ç¦»æ¢¯åº¦ï¼š</p><ul><li><code>state_values.detach()</code> ç¡®ä¿ Critic çš„æ¢¯åº¦ä¸å½±å“ Actor çš„æ›´æ–°ã€‚</li></ul></li><li><p>æ­£äº¤åˆå§‹åŒ–ï¼š</p><ul><li>ä¿æŒå‰å‘ä¼ æ’­ä¸­ä¿¡å·èŒƒæ•°ç¨³å®šï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ã€‚</li></ul></li></ol><table><thead><tr><th>ç›®æ ‡å†…å®¹</th><th>å®ç°æƒ…å†µè¯´æ˜</th></tr></thead><tbody><tr><td>å®ç°ç­–ç•¥æ¢¯åº¦ç®—æ³•</td><td>ä½¿ç”¨ REINFORCE æ–¹æ³•ï¼Œå«å¥–åŠ±æ ‡å‡†åŒ–ä¸ Monte Carlo å›æŠ¥ä¼°è®¡</td></tr><tr><td>å®ç° Actor-Critic ç®—æ³•</td><td>åŒ…å« actor-critic è”åˆç½‘ç»œï¼Œä½¿ç”¨ critic è¿›è¡Œä¼˜åŠ¿ä¼°è®¡</td></tr><tr><td>æ”¯æŒè½¨è¿¹å­˜å‚¨ä¸æŸå¤±è®¡ç®—</td><td>å®ç° log-probã€çŠ¶æ€ä»·å€¼ã€å¥–åŠ±çš„å­˜å‚¨ä¸å¤„ç†</td></tr><tr><td>å¥–åŠ±æŠ˜æ‰£ä¸æ ‡å‡†åŒ–å¤„ç†</td><td>ä½¿ç”¨ gamma=0.99 è®¡ç®—æŠ˜æ‰£å›æŠ¥å¹¶è¿›è¡Œæ ‡å‡†åŒ–</td></tr></tbody></table><h3><strong>2. Actor-Critic ç®—æ³•ï¼ˆ1é¡µPPTï¼‰</strong></h3><h4><strong>æ ¸å¿ƒæ€æƒ³</strong></h4><p>è”åˆè®­ç»ƒç­–ç•¥ç½‘ç»œï¼ˆActorï¼‰å’Œä»·å€¼ç½‘ç»œï¼ˆCriticï¼‰ï¼Œç”¨Criticçš„ (V(s)) å‡å°æ–¹å·®ã€‚</p><hr><h4><strong>å…³é”®å…¬å¼</strong></h4><p>ä¼˜åŠ¿å‡½æ•°ï¼š</p><p>$A(s,a) = R_t - V_\\phi(s)$</p><p>è”åˆæŸå¤±ï¼š</p><p>$\\mathcal{L} = -\\log \\pi_\\theta(a|s) \\cdot A(s,a) + 0.5 \\cdot (V_\\phi(s) - R_t)^2$</p><p>å…¶ä¸­ï¼š</p><ul><li>$\\pi_\\theta$æ˜¯Actorç½‘ç»œï¼Œ$V_\\phi$æ˜¯Criticç½‘ç»œ</li><li>$R_t$</li></ul><pre><code class="language-mermaid">flowchart TD
    A[state] --&gt; B[å…±äº«å±‚ state_dim --&gt; hidden_dim]
    B --&gt; C[action \\nhidden_dim --&gt; action_dim]
    B --&gt; D[critic\\nhidden_dim --&gt; 1]

</code></pre>`,91)])))}};export{s as category,h as date,u as default,w as summary,g as title,i as updated};
