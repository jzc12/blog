import{o as e,c as i,a}from"./index-7a689b3c.js";const l={class:"markdown-body"},d={__name:"1",setup(r,{expose:o}){return o({frontmatter:{}}),(c,t)=>(e(),i("div",l,t[0]||(t[0]=[a("<h1>Hi</h1><p>这是一篇示例文章，用来测试文章系统是否正常工作。</p><h2>Markdown 特性</h2><h3>4. <code>calculateLoss(self, gamma)</code></h3><ol><li><p>计算折扣回报：</p><ul><li>逆序遍历 <code>rewards</code>，按公式计算累积折扣回报：</li><li>$G_t = r_t + \\gamma \\cdot G_{t+1}$</li><li>结果存入 <code>discounted_rewards</code>。</li></ul></li><li><p>回报标准化：</p><ul><li>减去均值，除以标准差稳定训练。</li></ul></li><li><p>优势函数计算：</p><ul><li>$\\text{advantages} = \\text{discounted_rewards} - \\text{state_values}.detach()$ （<code>detach()</code> 阻止梯度流向 Critic，仅用于评估策略）。</li></ul></li><li><p>损失计算：</p><ul><li>策略损失（Actor）： $\\mathbb{E}[\\log \\pi(a|s) \\cdot \\text{advantages}]$</li></ul></li></ol><h3><strong>2. Actor-Critic 算法（1页PPT）</strong></h3><h4><strong>核心思想</strong></h4><p>联合训练策略网络（Actor）和价值网络（Critic），用Critic的 (V(s)) 减小方差。</p><hr><h4><strong>关键公式</strong></h4><p>优势函数：</p><p>$A(s,a) = R_t - V_\\phi(s)$</p><p>联合损失： $\\mathcal{L} = -\\log \\pi_\\theta(a|s) \\cdot A(s,a) + 0.5 \\cdot (V_\\phi(s) - R_t)^2$</p><p>其中：<br> - $\\pi_\\theta$是Actor网络，$V_\\phi$是Critic网络<br> - $R_t$</p>",14)])))}};export{d as default};
